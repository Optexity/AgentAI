agent_dir: "train_data"
html_data_config: "ComputerGYM/computergym/demonstrations/demonstration_config.yaml"

agent_name: hubspot_agent

models:
  - model_name_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct
    adapter_name_or_path: saves/
    template: llama3
    trust_remote_code: true
    context_length: 100000

  - model_name_or_path: Qwen/Qwen2.5-7B-Instruct-1M
    adapter_name_or_path: saves/
    template: qwen
    trust_remote_code: true
    context_length: 100000

  - model_name_or_path: Qwen/Qwen2.5-14B-Instruct-1M
    adapter_name_or_path: saves/
    template: qwen
    trust_remote_code: true
    context_length: 100000

train_config:
  ### method
  stage: sft
  do_train: true
  finetuning_type: lora
  lora_rank: 8
  lora_target: all

  ### dataset
  max_samples: 100000
  overwrite_cache: true
  preprocessing_num_workers: 16

  ### output
  logging_steps: 10
  save_steps: 74
  plot_loss: true
  overwrite_output_dir: true
  report_to: wandb

  ### train
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 1.0e-4
  num_train_epochs: 5
  lr_scheduler_type: cosine
  warmup_ratio: 0.1
  bf16: true
  ddp_timeout: 180000000

  ### eval
  # eval_dataset: alpaca_en_demo
  # val_size: 0.1
  # per_device_eval_batch_size: 1
  # eval_strategy: steps
  # eval_steps: 500

inference_config:
  infer_backend: vllm # choices: [huggingface, vllm]
  vllm_enforce_eager: true
